{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all imports\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "#import urllib2 (Python2)\n",
    "import urllib.request\n",
    "import bs4 #this is beautiful soup\n",
    "import time\n",
    "import operator\n",
    "import socket\n",
    "#import cPickle (Python2)\n",
    "import pickle\n",
    "import re # regular expressions\n",
    "\n",
    "from pandas import Series\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping data science skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What skills are in demand for data scientists?\n",
    "- Should we have a lecture on Spark or only on MapReduce?\n",
    "\n",
    "We want to scrape the information from job advertisements for data scientists from indeed.com\n",
    "Let's scrape and find out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get website and parse html\n",
    "Using `BeautifulSoup` the html code of the Dutch version of indeed.com is parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed url for job postings containing data scientist\n",
    "url = 'https://www.indeed.nl/jobs?q=data+scientist&l='\n",
    "# read the website\n",
    "source = urllib.request.urlopen(url).read()\n",
    "# parse html code\n",
    "bs_tree = bs4.BeautifulSoup(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job count\n",
    "Find the number of postings. Basically the search count is located on the 1st page of the website; use your browser's web developer tools to find the related tag and attributes. As this is a string we must cast it to an int taking into account possible thousands separators.<br><br>See also Remarks at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# August 2020 solution. Since 2015 Indeed.com altered the precise location of the total count of pages.\n",
    "result_string = bs_tree.find('div', attrs = {\"id\": \"searchCountPages\"}).text\n",
    "job_count_string = result_string.split(' ')[-2]\n",
    "print (\"Number of hits: \", job_count_string)\n",
    "# statements below to handle thousands separator\n",
    "job_count_digits = [int(d) for d in job_count_string if d.isdigit()]\n",
    "job_count = np.sum([digit*(10**exponent) for digit, exponent in \n",
    "                    zip(job_count_digits[::-1], range(len(job_count_digits)))])\n",
    "print (job_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job links\n",
    "The original CS109 2015 version assumes an exact amount of max 10 job postings per web page. At least on `indeed.nl` in August 2020 a variable number of job postings per page was encountered. So the number of pages calculation `num_pages = int(np.ceil(job_count/10))` is incorrect and cannot be used in the for loop. <br>\n",
    "The solution is to find all job postings per page (irrelevant if this is less or more than 10) in a while loop as long as the job_count is > 0. The job_count is decremented with number of jobs found on that page.<br><br>\n",
    "See also Remarks at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.indeed.nl\"\n",
    "job_links = []\n",
    "i = 0\n",
    "print (job_count)\n",
    "while job_count > 0:\n",
    "    job_postings = job_ids =[]\n",
    "    # first page\n",
    "    if i == 0:\n",
    "        url = 'https://www.indeed.nl/jobs?q=data+scientist&l='\n",
    "    else:\n",
    "        url = 'https://www.indeed.nl/jobs?q=data+scientist&start=' + str(i * 10)\n",
    "    html_page = urllib.request.urlopen(url).read()\n",
    "    bs_tree = bs4.BeautifulSoup(html_page)\n",
    "    job_link_area = bs_tree.find(id = 'resultsCol')\n",
    "    # list with all divs of class \"row result\" in it\n",
    "    job_postings = job_link_area.findAll(\"div\", class_=\"jobsearch-SerpJobCard unifiedRow row result\") \n",
    "    job_ids = [jp.get('data-jk') for jp in job_postings]\n",
    "    # then for each job_link\n",
    "    for id in job_ids:\n",
    "        job_links.append(base_url + '/rc/clk?jk=' + id)\n",
    "    time.sleep(1)\n",
    "    job_count -= len(job_ids)\n",
    "    print (\"We found a lot of jobs: \", len(job_links), \". Remaining jobcount: \", job_count)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some precautions to enable us to restart our search\n",
    "Write job_links to a file so it may be retrieved if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the scraped links\n",
    "with open('data/scraped_links.pkl', 'wb') as f:\n",
    "    pickle.dump(job_links, f)\n",
    "    \n",
    "# Read canned scraped links. 2nd arg should be `rb` read-binary\n",
    "with open('data/scraped_links.pkl', 'rb') as f:\n",
    "    job_links = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skill set\n",
    "The desired skills to count in our job_link pages is stored in dictionary with value 0 as initialized count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_set = {'mapreduce': 0, 'spark': 0, 'python': 0, 'r': 0, 'sql': 0, 'nosql': 0, 'sas': 0} \n",
    "# R is troublesome => in RE use raw string for \\b which matches empty string before begin or after end of R\n",
    "\n",
    "## write initialization into a file, so we can restart later\n",
    "with open('data/scraped_links_restart.pkl', 'wb') as f:\n",
    "    pickle.dump((skill_set, 0),f)\n",
    "## read dictionary and index    \n",
    "#with open('data/scraped_links_restart.pkl', 'rb') as f:\n",
    "#    skill_set, index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check progress\n",
    "Code might be run to see the number of web_links yet to go. Code comment: CS109 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code below does the trick, but could be optimized for speed if necessary\n",
    "# e.g. skills are typically listed at the end of the webpage\n",
    "# might not need to split/join the whole webpage, as we already know\n",
    "# which words we are looking for \n",
    "# and could stop after the first occurance of each word\n",
    "\n",
    "with open('data/scraped_links_restart.pkl', 'rb') as f:\n",
    "    skill_set, index = pickle.load(f)\n",
    "    print (\"How many websites still to go? \", len(job_links) - index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count skills\n",
    "Count skills in skill_set dictionary. That is take the key (a string) and increment the value by 1 on first occurrence of key. Before we do so, the html text is cleaned using a regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "for link in job_links[index:]:\n",
    "    counter +=1  \n",
    "    \n",
    "    try:\n",
    "        html_page = urllib.request.urlopen(link).read()\n",
    "    except urllib.request.HTTPError:\n",
    "        print (\"HTTPError:\")\n",
    "        continue\n",
    "    except urllib.request.URLError:\n",
    "        print (\"URLError:\")\n",
    "        continue\n",
    "    except socket.error as error:\n",
    "        print (\"Connection closed\")\n",
    "        continue\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    html_text = re.sub(\"[^a-z.+3]\",\" \", html_page.lower().decode('utf-8', errors=\"replace\")) # replace all but the listed characters\n",
    "    \n",
    "    for key in skill_set.keys():\n",
    "        if ' '+key+' ' in html_text: \n",
    "            skill_set[key] +=1\n",
    "            \n",
    "    if counter % 5 == 0:\n",
    "        print (len(job_links) - counter - index)\n",
    "        print (skill_set)\n",
    "        with open('data/scraped_links_restart.pkl','wb') as f:\n",
    "            pickle.dump((skill_set, index+counter),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize\n",
    "Present results in a bar graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseries = pd.Series(skill_set)\n",
    "pseries.sort_values(ascending=False)\n",
    "\n",
    "pseries.plot(kind = 'bar')\n",
    "## set the title to Score Comparison\n",
    "plt.title('Data Science Skills')\n",
    "## set the x label\n",
    "plt.xlabel('Skills')\n",
    "## set the y label\n",
    "plt.ylabel('Count')\n",
    "## show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Code is based on the [2015 CS109 solution](https://github.com/cs109/2015/blob/master/Lectures/02-DataScrapingQuizzes.ipynb).\n",
    "* some minor changes due to Python v2 vs v3 differences\n",
    "* the number of jobresults per page is varying.\n",
    "* quite a number of job links are duplicates. Might be due to advertisements.\n",
    "* skill in language R is given the above code hard to find. As a consequence the final R count is too high. A solution migt be the approach by  [Jesse Steinweg-Woods](https://jessesw.com/Data-Science-Skills/)\n",
    "* the searchCount is located on a different html location\n",
    "* the searchCount on the first page is not always reliable. Example of 18 August 2020: searchCount 7, but 10 job postings on page. Or searchCount 20 and 18 on page(s). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
